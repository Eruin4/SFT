{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output', 'url'],\n",
      "    num_rows: 1911\n",
      "})\n",
      "{'instruction': \"Extract the correct answer from the solution. A single integer.\\nsolution:\\nangle DEF\\\\). Let \\\\(R'\\\\) be the reflection of \\\\(Q\\\\) over \\\\(H\\\\). The homothety centered at \\\\(D\\\\) that maps the incircle to the \\\\(D\\\\)-excircle also maps \\\\(R'\\\\) to \\\\(P\\\\), implying that \\\\(D\\\\), \\\\(R'\\\\), and \\\\(P\\\\) are collinear, so \\\\(R' = R\\\\).\\n\\nTherefore, \\\\(\\\\frac{HQ}{HR} = 1\\\\).\\n\\nThe answer is \\\\(\\\\boxed{1}\\\\).\\nfinal answer: \", 'output': '1', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nered triples. If $a=1$, then we need $b+c=9$, which has 6 solutions for $b, c \\\\neq 1$; a similar argument for $b$ and $c$ gives a total of 18 such solutions. It is easy to check that all the solutions we found are actually solutions to the original equations. Adding, we find $18+3=21$ total triples.\\nfinal answer: ', 'output': '21', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\near that $X F E$ and $Y G E$ are congruent, so the area of $A X Y D$ is equal to that of $A F G D$. But $A F G D$ is simply a 12 by 6 rectangle, so the answer must be 72 . (Note: It is also possible to directly compute the values of $A X$ and $D Y$, then use the formula for the area of a trapezoid.)\\nfinal answer: ', 'output': '72', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n3 Let $a_{1}=a, a_{2}=b$; we successively compute $a_{3}=a+b ; \\\\quad a_{4}=a+$ $2 b ; \\\\quad \\\\ldots ; \\\\quad a_{10}=21 a+34 b$. The equation $2002=21 a+34 b$ has three positive integer solutions, namely $(84,7),(50,28),(16,49)$, and each of these gives a unique sequence.\\nfinal answer: ', 'output': '3', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n _{n \\\\rightarrow \\\\infty} \\\\sum_{i=0}^{2016}(-1)^{i} \\\\cdot \\\\frac{\\\\binom{n}{i}\\\\binom{n}{i+2}}{\\\\binom{n}{i+1}^{2}}=\\\\sum_{i=0}^{2016}(-1)^{i} \\\\cdot \\\\frac{(i+1)}{(i+2)}=1-\\\\sum_{i=2}^{2016} \\\\frac{(-1)^{i}}{i} \\\\approx \\\\ln (2) $$ Then $\\\\frac{1}{A} \\\\approx \\\\frac{1}{\\\\ln (2)} \\\\approx 1.44$, so the answer is 1 .\\nfinal answer: ', 'output': '1', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n chosen, then there are $\\\\binom{8}{k}$ ways to choose $k$ columns, and $\\\\binom{8}{k}$ ways to choose $k$ rows, and this would uniquely determine the set of squares selected. Thus the answer is $$\\\\sum_{k=1}^{8}\\\\binom{8}{k}\\\\binom{8}{k}=-1+\\\\sum_{k=0}^{8}\\\\binom{8}{k}\\\\binom{8}{k}=-1+\\\\binom{16}{8}=12869$$\\nfinal answer: ', 'output': '12869', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nThe summand is equal to $k$ if $k$ divides 2009 and 0 otherwise. Thus the sum is equal to the sum of the divisors of 2009, or 2394.\\nfinal answer: ', 'output': '2394', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n we have $t(t-1)(t-2)(t-3)$ divides $6N$ and in particular, $$t(t-1)(t-2)(t-3) \\\\leq 6N.$$ From this follows $$t(t-1)(t-2)(t-3) \\\\leq 6t(t^{3}+3t+3) \\\\frac{12}{t}+\\\\frac{7}{t^{2}}+\\\\frac{24}{t^{3}} \\\\geq 1.$$ Since $t \\\\geq 13$, $$\\\\frac{12}{t}+\\\\frac{7}{t^{2}}+\\\\frac{24}{t^{3}}<1,$$ which is a contradiction.\\nfinal answer: ', 'output': '420', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nLet $x$ be the length of $B H$. Note that quadrilateral $A B D E$ is cyclic, so by Power of a Point, $x(56-x)=$ $20 \\\\cdot 15=300$. Solving for $x$, we get $x=50$ or 6 . We must have $B H>H D$ so $x=50$ is the correct length.\\nfinal answer: ', 'output': '50', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nacteristic polynomial for this recursion is $t^{2}-t-2=0$, which has roots 2 and -1 . Thus, $$a_{n}=A \\\\cdot(-1)^{n}+B \\\\cdot 2^{n}$$ Then plugging in $n=1$ and $n=2$ gives $A=-\\\\frac{1}{3}$ and $B=\\\\frac{2}{3}$, so $$a_{n}=\\\\frac{2^{n+1}+(-1)^{n}}{3}$$ With $n=12$, this evaluates to our answer of 2731 .\\nfinal answer: ', 'output': '2731', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nas the first, the second rook has $5 \\\\times 5=25$ possible places, and similarly, the third rook has $4 \\\\times 4=16$ possible places. However, the rooks are indistinguishable, so there are 3! $=6$ ways to reorder them. Therefore, the number of arrangements is $\\\\frac{36 \\\\times 25 \\\\times 16}{6}=2400$.\\nfinal answer: ', 'output': '2400', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\ngs: treat each of these pairs of letters as a single merged symbol, which leaves 23 symbols to permute. Similarly, there are 22! ways to arrange the alphabet such that ABC and DEF both appear as contiguous substrings. Thus, \\\\(p_{1}=23!/ 26!\\\\) and \\\\(p_{2}=22!/ 26!\\\\), so the answer is \\\\(23!/ 22!=23\\\\).\\nfinal answer: ', 'output': '23', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n$ dominoes in each block, then $a_{1}+a_{2}+\\\\ldots+a_{20}=26$ and $a_{i}>0$ for all $1 \\\\leq i \\\\leq 20$. Therefore, from stars and bars, we find that there are \\\\binom{25}{6} ways to select the dominoes and thus the subset $S$. Surprisingly, \\\\binom{25}{6} is not too hard to compute and is just 177100.\\nfinal answer: ', 'output': '177100', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nSince the average of $2, x$ and 10 is $x$, then $\\\\frac{2 + x + 10}{3} = x$. Multiplying by 3, we obtain $2 + x + 10 = 3x$. Re-arranging, we obtain $x + 12 = 3x$ and then $2x = 12$ which gives $x = 6$.\\nfinal answer: ', 'output': '6', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n arranging 4 rows of 4 squares of side length 2, a square of side length 8 can be formed. Thus, $4 \\\\cdot 4=16$ squares can be arranged in this way. Since these smaller squares completely cover the larger square, it is impossible to use more $2 \\\\times 2$ squares, so 16 is the largest possible number.\\nfinal answer: ', 'output': '16', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nSimplifying, $4 x^{2}-3 x^{2}=x^{2}$. When $x=2$, this expression equals 4 . Alternatively, when $x=2$, we have $4 x^{2}-3 x^{2}=4 \\\\cdot 2^{2}-3 \\\\cdot 2^{2}=16-12=4$.\\nfinal answer: ', 'output': '4', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n5 greater than the sum of the odd-numbered terms. Thus, the sum of the even-numbered terms is $S+1005$. Since the sum of all of the terms equals the sum of the odd-numbered terms plus the sum of the even-numbered terms, then $S+(S+1005)=5307$ or $2S=4302$ or $S=2151$. Thus, the required sum is 2151.\\nfinal answer: ', 'output': '2151', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\nSince 10 students have black hair and 3 students have black hair and wear glasses, then a total of $10-3=7$ students have black hair but do not wear glasses.\\nfinal answer: ', 'output': '7', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n is a perfect square (16 = 4 \\\\times 4) so the answer is not 16. The sum of the digits of 26 is 8, which is not a prime number, so the answer is not 26. Since 14 is not a multiple of three, 14 is not a perfect square, and the sum of the digits of 14 is 1 + 4 = 5 which is prime, then the answer is 14.\\nfinal answer: ', 'output': '14', 'url': ''}\n",
      "{'instruction': 'Extract the correct answer from the solution. A single integer.\\nsolution:\\n) must yield a consistent integer given our setup with values:\\n\\\\[ n = 12k, \\\\quad d = 3k + 6 \\\\]\\n\\nBy properties of these graphs and adjusted \\\\( \\\\lambda = \\\\mu \\\\):\\n\\\\[ n = 36 \\\\]\\n\\nThis gives a possible solution where the parameters agree with the conditions stipulated in the problem. So:\\n\\\\[\\n\\\\boxed{36}\\n\\\\]\\n\\nfinal answer: ', 'output': '36', 'url': ''}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c05054e08384e85ae93abca235fec11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 04:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.908400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.705500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.597100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dshs-wallga/dshs/lib/python3.10/site-packages/peft/utils/save_and_load.py:241: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('home/dshs-wallga/pgh/qwen_finetuned/omni_2/tokenizer_config.json',\n",
       " 'home/dshs-wallga/pgh/qwen_finetuned/omni_2/special_tokens_map.json',\n",
       " 'home/dshs-wallga/pgh/qwen_finetuned/omni_2/vocab.json',\n",
       " 'home/dshs-wallga/pgh/qwen_finetuned/omni_2/merges.txt',\n",
       " 'home/dshs-wallga/pgh/qwen_finetuned/omni_2/added_tokens.json',\n",
       " 'home/dshs-wallga/pgh/qwen_finetuned/omni_2/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 필요에 따라 사용\n",
    "\n",
    "# Pretrained 모델 로드\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id   # 151643\n",
    "tokenizer.padding_side = \"left\"                   # 디코더 전용 모델 권장\n",
    "tokenizer.add_special_tokens({\n",
    "    'pad_token':'<|pad|>',\n",
    "    'additional_special_tokens':['<|eot_id|>']\n",
    "})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# CSV 파일 경로\n",
    "filename = \"/home/dshs-wallga/pgh/regacy/filtered_omni_math_train.csv\"\n",
    "# CSV 파일 로드 (예시 CSV 파일에는 '문제'와 '답' 컬럼이 있다고 가정)\n",
    "df = pd.read_csv(filename)\n",
    "df['prompt'] = (\n",
    "    \"Extract the correct answer from the solution. A single integer.\\n\"\n",
    "    \"solution:\\n\"\n",
    "    + df['solution'].fillna('').astype(str).str[-300:]\n",
    "    + \"\\nfinal answer: \"\n",
    ")\n",
    "df['answer'] = df['answer'].fillna('').astype(str)\n",
    "\n",
    "# 각 항목의 키를 변환하여 새로운 리스트 생성\n",
    "converted_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    new_item = {\n",
    "        \"instruction\": row.get(\"prompt\", \"\"),\n",
    "        \"output\": row.get(\"answer\", \"\"),\n",
    "        \"url\": \"\"  # URL 정보가 없으므로 빈 문자열로 채웁니다.\n",
    "    }\n",
    "    converted_data.append(new_item)\n",
    "\n",
    "# Hugging Face 데이터셋으로 변환\n",
    "train_dataset = Dataset.from_list(converted_data)\n",
    "\n",
    "# 확인\n",
    "print(train_dataset)\n",
    "for i in range(0, 1911, 100):\n",
    "    print(train_dataset[i])\n",
    "\n",
    "def preprocessing_data(examples):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    max_length = 512\n",
    "\n",
    "    for instr, resp in zip(examples['instruction'], examples['output']):\n",
    "        # 1) 인코딩\n",
    "        enc = tokenizer.apply_chat_template(\n",
    "            [{'role':'user','content':instr}],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        dec = tokenizer(resp + '<|eot_id|>', add_special_tokens=False)['input_ids']\n",
    "\n",
    "        # 2) 패딩 길이 계산\n",
    "        pad_len = max_length - len(enc) - len(dec)\n",
    "        if pad_len < 0:\n",
    "            dec = dec[: max_length - len(enc)]\n",
    "            pad_len = 0\n",
    "\n",
    "\n",
    "        # 3) input_ids, labels, attention_mask 생성\n",
    "        # Inside preprocessing_data after pad_len calculation and truncation\n",
    "        ids   = [tokenizer.pad_token_id] * pad_len + enc + dec\n",
    "        labs  = [-100] * pad_len + [-100] * len(enc) + dec\n",
    "        mask  = [0] * pad_len + [1] * (len(enc) + len(dec))\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        labels.append(labs)\n",
    "        attention_masks.append(mask)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocessing_data,\n",
    "    batched=True, num_proc=2,\n",
    "    remove_columns=['instruction','output','url']\n",
    ")\n",
    "train_dataset.set_format(type=\"torch\",\n",
    "                         columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=15,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy='steps',\n",
    "    label_names=['labels']\n",
    ")\n",
    "\n",
    "# Lora Tuning\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# LLM 풀튜닝 VRAM 30GB 사용\n",
    "# LoRA 사용시 18.6GB 사용됨 (RANK 16, 토큰임베딩 및 lm_head 학습 X)\n",
    "# 메모리 터질 시 RANK 사이즈 등 조정\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "save_directory = \"home/dshs-wallga/pgh/qwen_finetuned/omni_2\"\n",
    "lora_model.save_pretrained(\n",
    "    save_directory,\n",
    "    save_embedding_layers=True  # auto 대신 True로 직접 지정\n",
    ")\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged/tokenizer_config.json',\n",
       " '/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged/special_tokens_map.json',\n",
       " '/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged/vocab.json',\n",
       " '/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged/merges.txt',\n",
       " '/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged/added_tokens.json',\n",
       " '/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft        import PeftModel\n",
    "\n",
    "# 1) 베이스 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2) 토크나이저 로드 후 임베딩 크기 맞춤\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/dshs-wallga/pgh/qwen_finetuned/omni_2\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 3) 어댑터 로드\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"/home/dshs-wallga/pgh/qwen_finetuned/omni_2\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 4) 어댑터 병합\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# 5) 전체 모델 저장\n",
    "merged_model.save_pretrained(\"/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged\")\n",
    "tokenizer.save_pretrained(\"/home/dshs-wallga/pgh/qwen_finetuned/omni_2_merged\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dshs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
